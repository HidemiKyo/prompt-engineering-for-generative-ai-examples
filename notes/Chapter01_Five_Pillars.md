# Chapter 1: プロンプトの5つの原則

## 🔑 Key Concepts
- 方向性を示す
- 出力形式を指定する
- 例を示す
- 品質を評価する
- タスクを分割する

## 💡 Insights
- 方向性を示す
    - 背景情報（context）
        - 誰に向けた、何のための、どんな雰囲気で
    - Role Prompting
        - ex. スティーブ・ジョブズの命名スタイルでブレインストーミングしてください
    - PrewarmingまたはInternal Retrieval（内部検索）
        - モデルに「何を期待されているか」を理解させるために、前置きのプロンプトを使って“ウォームアップ”させる
    - タスクに対する助言をコンテキストとして挿入（プロンプト長増大によりコストが増える）
    - 方向性が少なすぎるとサンプル不足で生成できず、多すぎると創造性が狭まる
- 出力形式を指定する
    - ソフトウェアに組み込む際には、形式の指定と形式の誤りを確認する仕組み（ex.JSONパーサー）が必要
     - [structured-outputs](https://platform.openai.com/docs/guides/structured-outputs) で出力形式をJSONに固定可能
- 例を示す
    - 例はモデルの出力精度に大きく影響
    - 例の数が多いと精度が上がる
        - Zero-shot（例無し） < One-shot < Few-shot
    - 信頼性と創造性はトレードオフ
- 品質を評価する
    - 同じプロンプトを複数回試すことで、出力のばらつきや安定性を確認できる
        - [OpenAI Evals](https://github.com/openai/evals)という、LLMやそれを活用したシステムの性能を評価するためのフレームワークがある
        - 出力の精度、プロンプトの有効性、AIモデル間の比較などが検証可能
    - プログラムによってプロンプトを評価
        - 観点
            - 費用
                - 本番環境での継続的な運用コスト削減が可能か、を評価する
                - 評価（例）
                    1. プロンプトA：非常に長くて複雑（トークンが多い） → GPT-5（高価なモデル）でしか動かない → 1回の実行で数十円かかる
                    2. プロンプトB：短くて簡潔（トークンが少ない） → GPT-3.5（安価なモデル）でも動く → 1回の実行で数円
                    3. 本番運用では、プロンプトBの方が費用対効果が高い
                - エンドユーザー視点での費用対効果とは別の間隔
                    - 「このAI、1回の質問で月額料金に見合う価値ある？」という感覚
            - 遅延
                - AIに質問してから答えが返ってくるまでの「待ち時間」を遅延と定義
                - ユーザーが「遅い」「待たされる」と感じると、使いづらい・ストレスになる → ユーザー体験が悪化する
                - 評価（例）
                    1. プロンプトA：長文（トークンが多い）＋GPT-5（大規模なモデル） → 応答まで10秒 → ユーザーが離脱
                    2. プロンプトB：簡潔＋GPT-3.5 → 応答まで2秒 → スムーズで快適
                    3. プロンプトAは再設計が必要かもしれない
            - 呼び出し回数
                - タスクを完了するまでのプロンプト呼び出し回数を評価し、処理のスピードとコストを最適化するための指標とする
                - 評価（例）
                    - AIに「文章を要約して → 翻訳して → タグ付けして」と指示する処理の設計
                        - 設計A：for 文で AI を 5 回呼び出して、各ステップで部分的な処理を実行
                        - 設計B：1 回のプロンプトで「要約・翻訳・タグ付け」を一括処理するように指示
                        - 効率性の判断を「総コスト × 総時間」で行う
                - APIのレート制限や同時接続数：大量呼び出しが並列処理される場合、インフラ側の制約に引っかかる可能性が無いか合わせて確認する
                - 非同期処理の複雑化：呼び出しが多いと、エラー処理や順序制御が煩雑になる可能性が無いかも合わせて確認する
            - 性能（プロンプトの出力）
                - 外部フィードバックシステムを実装して、AI出力の性能を検証・改善
                - 外部フィードバックシステムの例
                    1. ユーザーがプロンプトを書く：「この材料で橋を作ったら安全ですか？」
                    2. 生成AIが回答する：「はい、安全です。」
                    3. 外部の物理演算エンジンで検証 → 「実は崩れる可能性が高い」
                    4. フィードバックをAIに返す → プロンプトや回答を改善
            - 分類の精度（プロンプトのラベル付け）
                - 別の生成AIモデルやルールベースのラベル付けと比較して、プロンプトのラベル付けの精度を測定
                - 分類の精度を比較する例
                    1. プロンプト：「このレビューはポジティブですか？」
                    2. AIの回答：「はい、ポジティブです。」
                    3. 別のAIモデルやルールベースの方法でも「ポジティブ」と判定
                    4. 一致したので「分類精度が高い」と判断、もし一致しなければ → 精度が低い → プロンプトやモデルを改善する必要あり
            - 推論の精度
                - どんな時にAIが間違えるのかを分析して、AIの推論の精度を測定
                - 推論の精度を分析する例
                    1. 問題：「ある商品を30%引きで買った後、さらに10%引きになると、最終価格はいくら？」
                    2. AIの回答：「30%引き → 10%引き → 合計40%引き」 →  間違い（割引は累積ではない）
                    3. 正しい計算：「元値 → 30%引き → その価格からさらに10%引き」 →  正解
                    4. このような事例を分析することで、AIが「割引の累積計算」に弱いと分かる
                - 推論強化手法
                    - Tree of Thoughts（ToT）
                    - self-consistency
            - 安全性
                - AIが危険な内容や不適切な回答を返してしまう場面を、事前に見つけて防ぐ仕組みを導入する
                    - OWASP Top 10 for LLMやMITRE ATLASなどの安全性評価フレームワークを使ったテンプレート設計
                    - LLMを利用する際のセキュリティリスク
                        - [OWASP Top 10 for LLM](https://genai.owasp.org/llm-top-10/) 
                        - [2025版 OWASP LLMアプリケーションのトップ10 全文翻訳](https://qiita.com/akiraokusawa/items/dcadb724e067233db569)
                    - AIシステムに特化したサイバー脅威や攻撃手法に関するフレームワーク
                        - [ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS)
                        - [MITRE ATLASとは何か？：概要編](https://www.trendmicro.com/ja_jp/jp-security/25/b/expertview-20250228-01.html)
                        - [生成AIに対するセキュリティ脅威と対策　第1回～MITRE ATLASにみる生成AIのセキュリティ脅威～](https://www.intellilink.co.jp/column/security/2025/082100.aspx)
                    - 例）
                        - AI連携モジュール：ChatGPT API（GPT-3.5）
                        - 安全フィルター：NGワード辞書＋出力検査ルール
                        - 検出システム：出力ログの自動スキャン＋アラート通知
                        - 承認フロー：AI出力 → 担当者レビュー → 承認 →公開
                        - ログ管理：SmartDBの操作履歴＋API呼び出し履歴

- タスクを分割する
    - 複雑な指示を段階的に処理させると、AIが混乱せず、各ステップでFBを反映しやすくなる

## 🧪 実験メモ
- GPT-4で「Dividing Labor」を試したプロンプト例と結果

## 📚 参考リンク
- [教材ノートブック](https://github.com/BrightPool/prompt-engineering-for-generative-ai-examples/tree/main/content)

